A demo system for running concurrent and distributed processes in
Python. Not yet usable.

Roadmap
=======

More examples

Distributed version (using regular spawn, send, monitor to remote
processes)

Networking lib similar to gen_tcp

Exe running lib focused on running all sorts of exes and pipelines,
possibly some expect-like support too (to be able to implement
automated tests for interactive exes)

Much more robust concurrency based test framework

Install
=======

install Python 3.10 or later
  -> before 3.10 is released you need the beta version
dependencies:
pip3 install python-prctl
pip3 install dill

Run the tests:

./test_framework.py -j1

(there is an issue running the sck module tests in parallel at the
moment, fixing it is lower priority)

Examples (TODO)
===============

Simple hello
------------

```
import occasional
from occasional import spawn, send, receive, slf

def say_hello():
    while True:
        match receive():
            case (frm, nm):
                send(frm, f"hello {nm}")

def f():
    h = spawn(say_hello)

    send(h, (slf(), "foo"))
    x = receive()
    assert(x == "hello foo")
    print(x)

    send(h, (slf(), "bar"))
    x = receive()
    assert(x == "hello bar")
    print(x)

if __name__ == "__main__":
    occasional.run(f)
```


running tasks in parallel

running "server" exes, native and non native
  restarting automatically when server exits
  logging exit information, timestamps
  monitoring

Theory
======

A concurrency/distributed framework demo based on some ideas from
Erlang, built in Python, uses Linux processes (and not Posix threads
or a green threads system).

The most critical aspect of the design is the reliable detection of
spawned process exit with the reason why - an arbitrary exit value if
the process produced a result, or error info if it failed.

* you spawn a new process using a Python function to run as the new
  process with minimal boilerplate
* you get an inbox with that process that can receive messages
* the system uses simple message passing with native values, you pass
  inboxes around as values which can then be sent to
* incoming messages are all multiplexed onto the same input queue so
  you don't explicitly deal with different connections
* processes in this system are Linux processes
* you can reliably detect when and why a process exits on the local
  machine, whatever the reason
* you can as reliably as possible detect the same for remote processes
* there's a ports system like Erlang to run non native processes
* it reproduces the key behaviour in Erlang that in addition to the
  single multiplexed inbox, you only have one thread reading from this
  queue
* the slightly subtler behaviour which makes a lot of things easier
  is that if one process sends several messages to another, the receiving
  process will receive them in order unless there's a fault
* a process is a unit of resource management recovery -> resources
  don't leak when a process exits or crashes
* it's designed to build crash only systems, sigkill is routinely used
  on user processes

It is a usable system in its own right, it's also designed to be a
demo of one way to use Linux processes and sockets to create a system
like this, that could be ported to other programming languages.

It doesn't try to emulate the very fast process spawning and very fast
message passing of Erlang.

Architecture
============

It uses Unix sockets to monitor processes. Every user process is a
Linux process. There is a central services process started by the
system, every process in the system has a unix socket connection to
this process. When you start a new process, it's the central services
process that ultimately arranges the fork (using forkserver in python
multiprocessing atm), and this central monitors the socket to detect
when the spawned processes exit. Connecting to other local spawned
processes is done via this central creating a socket pair and passing
one socket to each of the two connecting processes (the connect is
implicit when the first message is sent).



Code overview
=============

XX_tests
--------

Tests for the module XX

get_proc_socket_info.py
-----------------------

code to read /proc to see what socket connections the kernel reports
for processes

inbox.py
--------

Demo of some Erlang style inbox features:

send with an address, no explicit connection

all incoming messages are combined into one queue, you don't work with
separate connections from each sender

selective receive: get messages matching a predicate, unmatching
messages are left in the queue

you can send addresses in messages

multiprocessing_wrap.py
-----------------------

Simple wrapper around multiprocessing to make it easy to spawn
processes which use the multiprocessing infrastructure to pass sockets
when forking, and use the dill library to pickle everything else.

occasional.py
-------------

The main framework code. It uses the inbox, multiprocessing_wrap, sck,
spawn, and yeshup modules. The bulk of the code is the main loop in
the co-ordination process which handles spawn, monitor messages and
connecting processes to each other.

shared_port.py
--------------

Running a server server, which can launch new server processes,
clients can connect to these, using only a single listening port and
passing open socket ends between processes

spawn.py
--------

Running a process, then catching when it exits and getting successful
exit values and error information on failure

sck.py
------

Wrapper code around the low level sockets api adding safety and some
higher level features

test_framework.py
-----------------

A custom test framework for the system: didn't find any Python
test frameworks which met these requirements:

* test cases are flexible/ first class (maybe some have this, didn't
  find any info in any docs I looked at)
* ability to do test assertions concurrently in multiple processes

yeshup.py
---------

Simple code to implement a child process always exiting when the
parent exits
