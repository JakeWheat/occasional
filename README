A experiment in running concurrent processes in Python.

This experiment came to an end - I was eventually struggled too much
trying to do this sort of work in Python. The spiritual successor is
part of a new project, a programming language called Burdock:
https://github.com/JakeWheat/burdock/

Try it out
==========

install Python 3.10 or later
  -> before 3.10 is released you need the beta version
dependencies:
pip3 install python-prctl
pip3 install dill
pip3 install tblib
pip3 install setproctitle
pip3 install duckdb (used only in the tests at the moment)

Run the tests:

make test

Examples (TODO)
===============

Simple hello
------------

```
import occasional
from occasional import spawn, send, receive, slf

def say_hello():
    while True:
        match receive():
            case (frm, nm):
                send(frm, f"hello {nm}")

def f():
    h = spawn(say_hello)

    send(h, (slf(), "foo"))
    x = receive()
    assert(x == "hello foo")
    print(x)

    send(h, (slf(), "bar"))
    x = receive()
    assert(x == "hello bar")
    print(x)

if __name__ == "__main__":
    occasional.run(f)
```

Theory
======

A concurrency/distributed framework demo based on some ideas from
Erlang, built in Python, uses Linux processes (and not e.g. Posix
threads or a green threads system).

The original motivation and key feature is the reliable detection of
unexpected process exit with the reason why. It's evolved into
something that has a lot of other Erlang-like ideas.

Design goals:

* you spawn a new process with a Python function to run as the new
  process with minimal boilerplate
* you get an inbox with that process that can receive messages
* the system uses simple message passing with native values, you pass
  inbox addresses around as values which can then be sent to
* incoming messages are all multiplexed onto the same input queue so
  you don't explicitly deal with different connections
* processes in this system are Linux processes
* you can reliably detect when and why a process exits on the local
  machine, whatever the reason
* you can as reliably as possible detect the same for remote processes
* there's a ports system like Erlang to run non native processes
* it reproduces the key behaviour in Erlang that in addition to the
  single multiplexed inbox, you only have one thread reading from this
  queue
* the slightly subtler behaviour which makes a lot of things easier
  is that if one process sends several messages to another, the receiving
  process will receive them in order unless there's a fault
* a process is a unit of resource management recovery -> resources
  don't leak when a process exits or crashes
* it's designed to build crash only systems, sigkill is routinely used
  on user processes

It is a usable system in its own right, it's also designed to be a
demo of one way to use Linux processes and sockets to create a system
like this, that could be ported to other programming languages.

It doesn't try to emulate the very fast process spawning and very fast
message passing of Erlang.

Architecture
============

It uses Unix sockets to monitor processes. Every user process is a
Linux process. There is a central services process started by the
system, every process in the system has a unix socket connection to
this process.

When you start a new process, it's the central services process that
ultimately arranges the fork, and this central monitors the socket to
detect when the spawned processes exit.

Connecting to other local spawned processes is done via this central
services process. It creates a socket pair and passes one socket to
the connecting process and the connectee (the connect is implicit when
the first message is sent).

set_pdeathsig(signal.SIGKILL) is used so that any process spawned
within the system is sigkilled if the parent exits.

It's designed to work nice as a interactively run, foreground and
background process, e.g. in the foreground, stopping the main process
will stop all the others, ctrl-c will cause the top level process to
exit and trigger all the others to exit using pdeathsig instead of
sending ctrl-c to all of them.

Code overview
=============

occasional.py
-------------

The main framework code. The bulk of the code is the main loop in the
co-ordination process which handles spawn, monitor messages and
connecting processes to each other.

The support modules for occasional.py are in occ/. There are examples
in examples/. The tests are in tests/, and these use a test framework
in test_framework/test_framework.py.

inbox.py
--------

Demo of some Erlang style inbox features:

send with an address, no explicit connection

all incoming messages are combined into one queue, you don't work with
separate connections from each sender

selective receive: get messages matching a predicate, unmatching
messages are left in the queue

you can send addresses in messages

logging.py
----------

Some support code for using Python logging in this system.


sck.py
------

Wrapper code around the low level sockets api adding safety and some
higher level features

spawn.py
--------

Running a process, then catching when it exits and getting successful
exit values and error information on failure

sysquery.py
-----------

Code to read system information (mainly from /proc) and run sql
queries on it (it doesn't support much data to query).


utils.py

yeshup.py
---------

Simple code to implement a child process always exiting when the
parent exits

test_framework.py
-----------------

A custom test framework for the system: didn't find any Python
test frameworks which met these requirements:

* test cases are flexible/ first class (maybe some have this, didn't
  find any info in any docs I looked at)
* ability to do test assertions concurrently in multiple processes

